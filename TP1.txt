Atenção:
- Não edite este ficheiro em programas como Word e afins. Use exclusivamente um editor de texto simples. Em caso de dúvida, use o editor do Spyder.
- Não altere a estrutura deste ficheiro. Preencha as respostas apenas nos espaços respectivos (a seguir à tag R#:)
- Pode adicionar linhas no espaço para as respostas mas as respostas devem ser sucintas e directas.

QUESTÔES:

Q1: Considerando os dados fornecidos, explique a necessidade de standardizar os valores dos atributos.
R1: Comummente, precisamos de fazer a standardizaçao dos dados relativos às colunas dos atributos. Relativamente à coluna da classe, não faz grande sentido estar a fazer standardizaçao
porque o seu valor corresponde apenas a um booleano a dizer se a nota é verdadeira/falsa. Relativamente às colunas dos 4 atributos, como explicado na aula, convém fazermos a standardização 
dos valores porque dados com escalas muito diferentes podem causar instabilidades numéricas. Por exemplo, numa SVM, no caso de uma feature ter escalas muito superiores relativamente 
às outras features, tal pode ter demasiada influencia na posição do discriminante. Se por alguma razão fizermos standardizacao nas classes, iriamos apenas obter valores diferentes para as classes. Ou seja, iriamos mudar a classe 1
para a classe 0.23, por exemplo, o que apenas iria dificultar a implementação e cálculo das probabilidades de ser de uma classe ou de outra.


Q2: Explique como calculou os parâmetros para standardização e como os usou no conjunto de teste.
R2: Os parametros para a standardizacao do conjunto de treino, media e desvio padrao, foram calculados a partir da média e do desvio padrão dos atributos do conjunto de treino.
No caso do teste, tal foi feito também com a média e desvio padrão do conjunto de treino. Os parametros para a standardizacao do treino foram usados para a standardizacao do conjunto de teste
pois caso fossem feitas duas standardizações diferentes estariamos a assumir que as distribuicoes de ambos os conjuntos são iguais e não temos essa informação.


Q3: Explique como calculou a probabilidade a priori de um exemplo pertencer a uma classe (a probabilidade antes de ter em conta os valores dos atributos do exemplo) na sua implementação do classificador Naïve Bayes. Pode incluir um trecho relevante do código se ajudar a explicar.
R3: A probabilidade apriori de um exemplo pertencer a uma classe (antes de ter em conta os valores dos atributos desse exemplo) é calculada a partir do número de exemplos de cada classe
no set de treino sobre o numero de exemplos totais (de qualquer classe) no set de treino. Corresponde à probabilidade com que começamos os cálculos -> P(X) = #elemsX/#totalElems, em que X é uma classe. 
Esta probabilidade é importante pois corresponde à probabilidade de uma classe antes de começarmos a fazer observacoes, é o valor mais aproximado que podemos calcular à priori.
O nosso objetivo final é calcular a probabilidade condicional P(X|a1, a2...) = P(X) * P(a1|X) * P(a2|X) * ... (que comecará com a probabilidade a priori, P(X)) que corresponde à probabilidade de um elemento com atributos conhecidos 
(a1,a2...) pertencer a uma classe (X). A probabilidade condicional calculada corresponde à probabilidade posterior, que já tem em conta as observações feitas.


Q4: Explique como o seu classificador Naïve Bayes prevê a classe a que um exemplo de teste pertence. Pode incluir um trecho relevante do código se ajudar a explicar.
R4: Um classificador Naïve Bayes começa por calcular um Kernel Density Estimation (KDE) para cada feature, para cada classe. Ou seja, vamos obter uma 
estimativa de densidade para cada feature para a classe 1 e para a classe 2.

Se por exemplo, só existisse uma feature, iriamos obter 2 KDEs, um para a classe 1 e outro para a classe 2. Para classificarmos um exemplo de teste,
calculariamos a probabilidade do teste ter a sua feature e de ser da classe x:
P(c1) = Prob de classe 1 apriori
P(x1) = Prob de feature do teste através do kde correspondente
Calculariamos P(x1, c1) = p(x1 | c1) * p(c1) que simplificado com logaritmos seria = ln (x1 | c1) + ln p(c1)
A probabilidade p(x1 | c1) seria dada pelo KDE pois estimámos previamente a densidade daquela feature na classe 1.

Este caso seria para 1 feature, que é um cenário demasiado simples. O que acontece com um Bayes Classifier é que o número de exemplos
de treino que necessitamos para obter uma amostra suficientemente boa cresce exponencialmente com o número de features. Com um classificador Naïve Bayes
fazemos a assumpção de que todas as features são condicionalmente independentes umas das outras, simplificando os cálculos e diminuindo imensamente o
número total de data points necessários para obter uma boa previsão de cada classe para dados que não foram previamente vistos.

Utilizando esta assumpção, após calcularmos um KDE para cada feature, para cada classe, vamos poder calcular a probabilidade de um dado ponto
(x1, x2, ..., xn) pertencer à classe 1 e à classe 2 da seguinte forma:
Probabilidade de pertencer à classe 1 = P(c1) * P(x1 | c1) * P(x2 | c1) * ... * P(xn | c1)
Simplificando com logaritmos obtemos:  ln P(c1) + ln P(x1 | c1) + ln P(x2 | c1) + ... + ln P(xn | c1), e fazendo o mesmo para a classe 2.
De seguida comparamos a probabilidade de pertencer à classe 1 e a probabilidade de pertencer à classe 2 e classificamos o ponto escolhendo a previsão
através de qual classe o ponto é mais provável pertencer.


Q5: Explique que efeito tem o parâmetro de bandwidth no seu classificador.
R5: A bandwith representa uma janela de dados que o KDE terá em conta para estimar as densidades seguindo uma distribuicao normal. Caso essa bandwith seja "larga", o número de dados que entram para o calculo dessa estimativa são
muitos mais e o resultado é uma curva muito suave. Pelo contrário, se a bandwith for muito "estreita", o número de pontos que entram para a estimativa sao muito poucos e isto faz com que a curva resultante
seja muito rugosa e instável, representando uma situaçao de overfit. Com isto, se a bandwith for muito pequena, o erro de treino será baixo e o de teste muito elevado. Se for muito grande, o erro de treino aumentará e o de
teste diminuirá comparativamente ao bandwith pequeno. Para isto, tem de se encontrar um balanço no valor dado. Resumidamente:
Se o bandwith for alto a curva será demasiado suave e vamos perder muita informação da distribuição por estarmos a generalizar demasiado. Está a acontecer underfitting.
Se o bandwith for baixo a curva será muito rugosa, ou seja, vai estar muito próximo de cada ponto que usámos para estimar a densidade. Neste caso acontece overfitting.
O que queremos é encontrar o bandwith que aproxima o KDE da distribuição cujos dados que temos seguem. Como não conhecemos a distribuição testamos vários bandwiths e escolhemos o que tem menor erro
de validação.


Q6: Explique que efeito tem o parâmetro gamma no classificador SVM.
R6: O parametro gamma representa a importancia que o kernel dá a pontos próximos ou, por outras palavras, quão longe um exemplo de treino pode influenciar. 
Para valores elevados, o kernel dará muita importancia a valores próximos o que fará com que o threshold gerado entre as duas classes seja muito rugoso e instável. 
Assim, um valor elevado diz-nos que a influencia de um exemplo vai ser "perto" desse exemplo. Para valores baixos de gamma, a influencia será mais distante o que 
fará com que a threshold seja mais fluida e não se adapte tanto aos exemplos. De modo a percebermos o que isto resulta nos valores de erro, com um valor de gamma 
baixo ambos os erros são elevados e com o aumentar deste gamma, ou seja, com a diminuição da distância de "influencia" o erro de treino desce, isto porque também 
começa a haver uma situação de overfit e não apenas pela variação do valor de gamma. Relativamente ao erro de validacao, este mantém-se aproximadamente igual
para todos os gammas.


Q7: Explique como determinou o melhor parâmetro de bandwidth e gamma para o seu classificador e o classificador SVM. Pode incluir um trecho relevante do código se ajudar a explicar.
R7: Para ambas as situações, os melhores parâmetros foram escolhidos com cross validation utilizando o training set. Escolhemos vários valores possíveis para estes parâmetros e
usámos cada um desses valores para treinar um classificador (5 vezes por cada valor). De seguida calculámos o validation error de cada fold e fizemos uma média dos erros. 
O parâmetro que gerou uma média de menor erro de validacao foi o escolhido.


Q8: Explique como obteve a melhor hipótese para cada um dos classificadores depois de optimizados os parâmetros.
R8: Depois de otimizados os parâmetros fizemos fit dos classificadores com os parâmetros escolhidos, utilizando desta vez o training set na sua totalidade (não utilizando k-folds). 
Desta forma obtemos a melhor hipótese para os classificadores, que posteriormente iremos aplicar ao test set para estimar o erro verdadeiro.


Q9: Mostre os melhores valores dos parâmetros optimizados, a estimativa do erro verdadeiro de cada uma das hipóteses que obteve (o seu classificador e os dois fornecidos pela biblioteca),
 os intervalos do número esperado de erros dados pelo teste normal aproximado e os valores dos testes de McNemar e discuta o que pode concluir daí.
R9: Naive Bayes => Melhor Bandwith = 0.28, Erro de teste =  0.0601
Gaussian Naive Bayes => Erro de teste = 0.095
SVM => Melhor gamma = 3.4, Erro de teste = 0.0043

IC Naive Bayes 95%Confianca => [59.5 ; 92.5]
IC Gaussian Naive Bayes 95%Confianca => [97.75 ; 138.3]
IC SVM 95%Confianca => [39.1 ; 66.96]

McNemar Naive Bayes vs GaussianNB = 31.13
McNemar Naive Bayes vs SVM = 8.49
McNemar GaussianNB vs SVM =  39.01
De acordo com os intervalos de confianca do teste normal aproximado, podemos observar que dois dos intervalos se interecetam. 
O intervalo de confianca do método de naive bayes e o do SVM intercetam-se. Isto representa que não podemos rejeitar a hipótese nula (terem performances idênticas) 
por isso não conseguimos concluir que um é melhor que o outro. Relativamente aos outros intervalos, Naive Bayes com Gaussian Naive bayes e SVM com Gaussian Naive Bayes,
não há intercecao. Isto significa que podemos dizer com 95% de confiança que o SVM e o Naive Bayes é melhor que o Gaussian Naive Bayes. 
Assim, de acordo com os intervalos podemos dizer com 95% de confianca que os modelos ordenados do melhor para o pior são SVM,NB > GNB.
Relativamente aos testes de mcnemar, caso os valores sejam superiores a 3.84 (valor da distribuicao chi quadrado com 1 grau de liberdade) então podemos
rejeitar a hipotese nula (2 classificadores são identicos e erram para as mesmas observações). Visto que todos são acima de 3.84 podemos rejeitar a hipotese nula e 
concluir com 95% de confianca que existe uma diferença significativa entre todos os classificadores embora não se consiga concluir a partir deste teste qual o melhor,
isto é, funcionam de forma diferente. Assim, pelo teste aproximado da normal conseguimos concluir que a performance do Naive Bayes e do SVM são melhores que o do
Gaussian Naive Bayes com 95% de confiança.
Pelos testes de mcnemar podemos concluir com 95% de confianca que os classificadores são todos diferentes entre si.

Q10: (Opcional) Mostre a estimativa do erro verdadeiro do classificador SVM optimizado (se fez a parte opcional do trabalho) e discuta se valeu a pena fazer essa optimização. Se não fez a parte opcional do trabalho deixe esta resposta em branco.
R10: Usando o teste aproximado da normal para calcular os intervalos de confiança para o SVM com o C = 1 e o melhor gamma vs SVM otimizado com os melhores parametros tanto para C como para gamma (escolhidos atraves 
de cross validation que resultaram num menor erro de validacao), os intervalos foram:
SVM normal: 51.0 +- 13.707981905554707
SVM otimizado: 53.0 +- 13.962492177643137

Test Error SVM: 0.0431
Test Error SVM Otimizado: 0.0424

A combinação de parâmetros que resultou num menor erro na cross validation foi C = 31 e Gamma = 0.043 logo foram estes os parâmetros escolhidos para realizar a previsão (predict) sobre o conjunto de teste.
Visto que estes intervalos são praticamente iguais e, como podemos ver, claramente se intersetam em todo o domínio, não foi vantajoso fazer esta otimização. Para além disto, os erros de teste calculados foram praticamente
iguais em ambas as versões, comprovando assim não ter sido frutífero fazer a otimizacao.

